[core]
# The folder where your airflow dags are stored
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
remote_logging = False

# The executor class that airflow should use
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database.
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@supabase-db:5432/airflow

# Number of seconds after which a DAG file is re-parsed
min_file_process_interval = 30

# Number of processes to use for dag parsing
# Increase this to parse DAG files faster (including subfolders)
# Recommended: Set to number of CPU cores available
parsing_processes = 4

# How long before timing out a python file import
dagbag_import_timeout = 30

# Whether to scan dag files in subdirectories
# This should be True by default, but we're making it explicit
dag_dir_list_interval = 10

# The number of retries each task is going to have by default
default_task_retries = 1

# Whether to load the DAG examples that ship with Airflow
load_examples = False

# Whether to enable pickling for xcom (needed for communication between tasks)
enable_xcom_pickling = True

[logging]
# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
remote_logging = False

[scheduler]
# Task instances listen for external kill signal (when you `airflow tasks clear` them),
# this defines the frequency at which they should listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the executor's queue
# to see if there's room, and try to find runnable tasks)
scheduler_heartbeat_sec = 5

# The number of times to try to schedule each DAG file
num_runs = -1

# How often should stats be printed to the logs
print_stats_interval = 30

# If set to True, serialized DAG representation is used
use_job_schedule = True

# Number of seconds after which a DAG file is re-parsed
min_file_process_interval = 30

# Number of seconds after which a DAG can be triggered
dag_dir_list_interval = 10

# How often should the scheduler check for orphaned tasks and SchedulerJobs
orphaned_tasks_check_interval = 300

# Length of time after which the scheduler terminates old DagFileProcessor processes
dag_file_processor_timeout = 50

# Number of DagFileProcessorManager processes
parsing_processes = 4

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_cert = 
web_server_ssl_key = 

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time. When set to 0, worker refresh is disabled.
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers.
worker_refresh_interval = 6000

# Secret key used to run your flask app
secret_key = temporary_key

# Number of workers to run the Gunicorn web server
workers = 4

# The worker class gunicorn should use
worker_class = sync

# Log files for the gunicorn webserver
access_logfile = -
error_logfile = -

# Expose the configuration file in the web server
expose_config = False

# Set to true to turn on authentication
authenticate = False

# Filter the list of dags by owner name (requires authentication to be enabled)
filter_by_owner = False

# Default DAG view
default_dag_view = graph

[api]
# Indicates whether the API server will allow unauthenticated access
auth_backends = airflow.api.auth.backend.basic_auth

[operators]
# The default owner assigned to each new operator, unless provided explicitly or passed via `default_args`
default_owner = airflow

[email]
email_backend = airflow.providers.sendgrid.utils.emailer.send_email

[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_user = airflow
smtp_password = airflow
smtp_port = 587
smtp_mail_from = airflow@example.com